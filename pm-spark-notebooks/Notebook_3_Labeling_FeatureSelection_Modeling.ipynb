{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #3\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Load Result from Notebook #2](#Load-Result-from-Notebook-#2)\n",
    "- [Label Construction](#Label-Construction)\n",
    "- [Feature Reduction](#Feature-Reduction)\n",
    "- [Prepare Train and Test Data](#Prepare-train-and-test-data)\n",
    "   - [Time-dependent Splitting](#Prepare-train-and-test-dataset-using-time-split-method)\n",
    "   - [Down Sample Negative Examples](#Down-Sample-Negative-examples:)\n",
    "   - [Cache Results](#Cache-results)\n",
    "- [Binary Classification Models](#Binary-Classification-Models:)\n",
    "   - [Random Forest classifier](#Random-Forest-classifier)\n",
    "   - [Gradient-Boosted Tree classifier](#Gradient-Boosted-Tree-classifier)\n",
    "   - [Hyper-Parameter Tuning & Cross Validation](#Hyper-Parameter-Tuning-&-Cross-Validation)\n",
    "   \n",
    " <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import atexit\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col,udf,lag,date_add,explode,lit,concat,unix_timestamp,sum, abs\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.dataframe import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, RFormula\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from pyspark.sql.functions import month, weekofyear, dayofmonth\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV, ParameterGrid\n",
    "\n",
    "start_time = time.time()\n",
    "base_path = \"/home/jovyan/data/\"\n",
    "suffix = \"_1000\"\n",
    "\n",
    "spark = ( SparkSession\n",
    "  .builder\n",
    "  .master(\"local[30]\")\n",
    "  .appName(\"pm\")\n",
    "  .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark.conf.set(\"spark.executor.memory\", \"Xg\")\n",
    "spark.conf.set(\"spark.driver.memory\", \"Xg\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Result from Notebook #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load result from Notebook #2 (\"FeatureEngineering_RollingCompute\")\n",
    "df = spark.read.parquet(base_path + 'tmp/notebook2_result' + suffix + '.parquet')\n",
    "\n",
    "# check the dimension of the dataset and make sure things look right\n",
    "print(df.count(), len(df.columns))\n",
    "df.select('key','deviceid').show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Construction\n",
    "\n",
    "For predictive maintenance use cases, we usually want to predict failure/problem ahead of time. In our example, we would like to be able to predict machine problem 7 days (failure prediction time window) in advance. That means for the label column, we need to label all the 7 days before the actual failure/problem day as \"1\". This time window should be picked based on the specific business case: in some situations it may be enough to predict failures hours in advance, while in others days or even weeks might be needed to make meaningful business decision such as allowing enough time for arrival of replacement parts.\n",
    "\n",
    "To find more detailed information about the label construction technique, please visit [this link](https://pdfs.semanticscholar.org/284d/f4ec85eed338a87fece985246c5bd4f56495.pdf).\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------ Create label column ------------------------------------#\n",
    "\n",
    "# Step 1: \n",
    "df = df.withColumn('label_tmp', col('problemreported')) \n",
    "\n",
    "# Step 2:\n",
    "wSpec = Window.partitionBy('deviceid').orderBy(df.date.desc())\n",
    "lag_window = 7  # Define how many days in advance we want to predict failure\n",
    "\n",
    "for i in range(lag_window):\n",
    "    lag_values = lag(df.label_tmp, default=0).over(wSpec)\n",
    "    df = df.withColumn('label_tmp', F.when((col('label_tmp')==1) | (lag_values==None) | (lag_values<1) | (lag_values>=(lag_window+1)), col('label_tmp')).otherwise(lag_values+1))\n",
    "\n",
    "# check the results\n",
    "print(df.select('label_tmp').distinct().rdd.map(lambda r: r[0]).collect()) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 3:\n",
    "### please note that we need to make \"label\" column double instead of integer for the pyspark classification models \n",
    "df = df.withColumn('label', F.when(col('label_tmp') > 0, 1.0).otherwise(0.0))\n",
    "df.createOrReplaceTempView(\"df_view\") \n",
    " \n",
    "# Step 4:\n",
    "df.orderBy('deviceid', 'date').select('deviceid', 'date', 'problemreported', 'label_tmp', 'label').show(20) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of \"label\" column\n",
    "df.select('label').describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction\n",
    "-  There are not many packages for feature selection in PySpark 2.0.2.,so we decided to use PCA to reduce the demensionality.\n",
    "-  There are so many features especially rolling features, we need to perform feature selection to reduce the feature set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the number of rolling features\n",
    "len([col_n for col_n in df.columns if '_rolling' in col_n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Use RFormula to create the feature vector\n",
    "rolling_features = list(s for s in df.columns if \"_rolling\" in s)\n",
    "formula = RFormula(formula = \"~\" + \"+\".join(rolling_features))\n",
    "output = formula.fit(df).transform(df).select(\"key\",\"features\") \n",
    "\n",
    "\n",
    "# Step 2 \n",
    "# Before PCA, we need to standardize the features, it is very important...\n",
    "# We compared 1) standardization, 2) min-max normalization, 3) combintion of standardization and min-max normalization\n",
    "# In 2), the 1st PC explained more than 67% of the variance\n",
    "# 1) & 3) generate exactly the same results for model.explainedVariance. \n",
    "# That means min-max normalization does not help in our case\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(output)\n",
    "\n",
    "\n",
    "# Step 3\n",
    "pca = PCA(k=50, inputCol=\"scaledFeatures\", outputCol=\"pca_roll_features\")\n",
    "model = pca.fit(scaledData)\n",
    "result = model.transform(scaledData).select(\"key\",\"pca_roll_features\")\n",
    "print(model.explainedVariance)\n",
    "\n",
    "\n",
    "# Step 4\n",
    "df = df.join(result, 'key', 'inner')\n",
    "rolling_drop_list = [col_n for col_n in df.columns if '_rolling' in col_n]\n",
    "df = df.select([column for column in df.columns if column not in rolling_drop_list])\n",
    "\n",
    "df.select('key','pca_roll_features').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of input columns for downstream modeling\n",
    "\n",
    "input_features2 = [item for item in df.columns if item not in [\"key\", \"deviceid\", \"date\", \"categorical_1\", \"categorical_2\", \"categorical_3\", \"categorical_4\", \"cat1\", \"cat2\", \"cat3\", \"cat4\"]]\n",
    "input_features = [item for item in input_features2 if not item.endswith(\"_category\") and not item.startswith(\"fault_code_type\")]\n",
    "\n",
    "test = [\n",
    " 'usage_count_1',\n",
    " 'usage_count_2',\n",
    " 'problem_type_1',\n",
    " 'problem_type_2',\n",
    " 'problem_type_3',\n",
    " 'problem_type_4',\n",
    " 'error_count_1',\n",
    " 'error_count_2',\n",
    " 'error_count_3',\n",
    " 'error_count_4',\n",
    " 'error_count_5',\n",
    " 'error_count_6',\n",
    " 'error_count_7',\n",
    " 'error_count_8',\n",
    " 'month',\n",
    " 'weekofyear',\n",
    " 'dayofmonth',\n",
    " 'warn_type1_total',\n",
    " 'warn_type2_total',\n",
    " 'fault_code_type_1_count',\n",
    " 'fault_code_type_2_count',\n",
    " 'fault_code_type_3_count',\n",
    " 'fault_code_type_4_count',\n",
    " 'problem_type_1_per_usage1',\n",
    " 'problem_type_2_per_usage1',\n",
    " 'problem_type_3_per_usage1',\n",
    " 'problem_type_4_per_usage1',\n",
    " 'fault_code_type_1_count_per_usage1',\n",
    " 'fault_code_type_2_count_per_usage1',\n",
    " 'fault_code_type_3_count_per_usage1',\n",
    " 'fault_code_type_4_count_per_usage1',\n",
    " 'problem_type_1_per_usage2',\n",
    " 'problem_type_2_per_usage2',\n",
    " 'problem_type_3_per_usage2',\n",
    " 'problem_type_4_per_usage2',\n",
    " 'fault_code_type_1_count_per_usage2',\n",
    " 'fault_code_type_2_count_per_usage2',\n",
    " 'fault_code_type_3_count_per_usage2',\n",
    " 'fault_code_type_4_count_per_usage2',   \n",
    " 'problem_type_1_category_encoded',\n",
    " 'problem_type_2_category_encoded',\n",
    " 'problem_type_3_category_encoded',\n",
    " 'problem_type_4_category_encoded',\n",
    " 'problem_type_1_per_usage1_category_encoded',\n",
    " 'problem_type_2_per_usage1_category_encoded',\n",
    " 'problem_type_3_per_usage1_category_encoded',\n",
    " 'problem_type_4_per_usage1_category_encoded',\n",
    " 'problem_type_1_per_usage2_category_encoded',\n",
    " 'problem_type_2_per_usage2_category_encoded',\n",
    " 'problem_type_3_per_usage2_category_encoded',\n",
    " 'problem_type_4_per_usage2_category_encoded',\n",
    " 'fault_code_type_1_count_category_encoded',\n",
    " 'fault_code_type_2_count_category_encoded',\n",
    " 'fault_code_type_3_count_category_encoded',\n",
    " 'fault_code_type_4_count_category_encoded',\n",
    " 'fault_code_type_1_count_per_usage1_category_encoded',\n",
    " 'fault_code_type_2_count_per_usage1_category_encoded',\n",
    " 'fault_code_type_3_count_per_usage1_category_encoded',\n",
    " 'fault_code_type_4_count_per_usage1_category_encoded',\n",
    " 'fault_code_type_1_count_per_usage2_category_encoded',\n",
    " 'fault_code_type_2_count_per_usage2_category_encoded',\n",
    " 'fault_code_type_3_count_per_usage2_category_encoded',\n",
    " 'fault_code_type_4_count_per_usage2_category_encoded',\n",
    " 'cat1_encoded',\n",
    " 'cat2_encoded',\n",
    " 'cat3_encoded',\n",
    " 'cat4_encoded',     \n",
    " 'pca_1_warn',\n",
    " 'pca_2_warn',\n",
    " 'pca_3_warn',\n",
    " 'pca_4_warn',\n",
    " 'pca_5_warn',\n",
    " 'pca_6_warn',\n",
    " 'pca_7_warn',\n",
    " 'pca_8_warn',\n",
    " 'pca_9_warn',\n",
    " 'pca_10_warn',\n",
    " 'pca_11_warn',\n",
    " 'pca_12_warn',\n",
    " 'pca_13_warn',\n",
    " 'pca_14_warn',\n",
    " 'pca_15_warn',\n",
    " 'pca_16_warn',\n",
    " 'pca_17_warn',\n",
    " 'pca_18_warn',\n",
    " 'pca_19_warn',\n",
    " 'pca_20_warn',\n",
    " 'pca_roll_features'\n",
    "]\n",
    "\n",
    "label_var = ['label']\n",
    "key_cols =['key','deviceid','date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "df = va.transform(df).select('deviceid','date','label','features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maxCategories so features with > 10 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to do “StringIndexer” on the label column, fit on the entire dataset to include all labels in index. Also, the label column has to be Double instead of Integer type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train and test dataset using time split method \n",
    "-  training data: year 2012-2014\n",
    "-  testing data: year 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = df.filter(df.date > \"2011-12-31\").filter(df.date < \"2015-01-01\")\n",
    "testing = df.filter(df.date > \"2014-12-31\")\n",
    "\n",
    "print(training.count())\n",
    "print(testing.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## show the distribution of label \"0\" and \"1\"\n",
    "df.groupby('label').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-Sample Negative examples:\n",
    "-  This is a ***highly umbalanced data*** with way more label \"0\" than \"1\" (\"1\" only accounts for 1.5%).\n",
    "-  So we need to down sample the negatives while keeping all positive samples.\n",
    "-  To make label \"1\" to \"0\" ratio close to 1:10 (you can use other ratio for example 1:5), we need to down-sample the \"0\"s (take 13.5% of all the label \"0\"s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SampleBy returns a stratified sample without replacement based on the fraction given on each stratum\n",
    "train_downsampled = training.sampleBy('label', fractions={0.0: 0.135, 1.0: 1.0}, seed=123).cache()\n",
    "train_downsampled.groupby('label').count().show()\n",
    "\n",
    "testing.groupby('label').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache results \n",
    "\n",
    "Do it when necessary especially if your downstream work (e.g. recursive modeling) use that data over and over again. Here in our case, after the train and test datasets are prepared, we cache them in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache datasets in memory\n",
    "train_downsampled.cache()\n",
    "testing.cache()\n",
    "\n",
    "# check the number of devices in training and testing data\n",
    "print(train_downsampled.select('deviceid').distinct().count())\n",
    "print(testing.select('deviceid').distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model storage directory path. This is where models will be saved.\n",
    "modelDir = base_path + 'model/'; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Models:\n",
    "-  Random Forest classifier\n",
    "-  Gradient-Boosted Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=100)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_rf = pipeline_rf.fit(train_downsampled)\n",
    "\n",
    "# Save model\n",
    "datestamp = str(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "rf_fileName = \"RandomForest_\" + datestamp;\n",
    "rfDirfilename = modelDir + rf_fileName + suffix;\n",
    "model_rf.save(rfDirfilename)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_rf = model_rf.transform(testing)\n",
    "predictions_rf.groupby('indexedLabel', 'prediction').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictionAndLabels = predictions_rf.select(\"indexedLabel\", \"prediction\").rdd\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Area under ROC = %g\" % metrics.areaUnderROC)\n",
    "print(\"Area under PR = %g\\n\" % metrics.areaUnderPR)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\")\n",
    "print(\"Accuracy = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"accuracy\"}))\n",
    "print(\"Weighted Precision = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "print(\"Weighted Recall = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"weightedRecall\"}))\n",
    "print(\"F1 = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"f1\"}))\n",
    "\n",
    "# PLOT ROC curve after converting predictions to a Pandas dataframe\n",
    "%matplotlib inline\n",
    "predictions_rf_pddf = predictions_rf.select('indexedLabel','probability').toPandas()\n",
    "labels = predictions_rf_pddf[\"indexedLabel\"]\n",
    "prob = []\n",
    "for dv in predictions_rf_pddf[\"probability\"]:\n",
    "    prob.append(dv.values[1])\n",
    "     \n",
    "fpr, tpr, thresholds = roc_curve(labels, prob, pos_label=1.0);\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark MulticlassClassificationEvaluator in version 2.0.2 only gives weighted precision and recall. But we would also like to see the raw precision and recall as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn\n",
    "rf_result = predictions_rf.select('indexedLabel', 'prediction').toPandas()\n",
    "\n",
    "rf_label = rf_result['indexedLabel'].tolist()\n",
    "rf_prediction = rf_result['prediction'].tolist()\n",
    "\n",
    "precision, recall, fscore, support = score(rf_label, rf_prediction)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-Boosted Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxDepth=10, minInstancesPerNode=5, maxIter=50)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline_gbt = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_gbt = pipeline_gbt.fit(train_downsampled)\n",
    "\n",
    "# save model\n",
    "datestamp = str(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "gbt_fileName = \"GradientBoostedTree_\" + datestamp;\n",
    "gbtDirfilename = modelDir + gbt_fileName + suffix;\n",
    "model_gbt.save(gbtDirfilename)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_gbt = model_gbt.transform(testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show prediction results\n",
    "predictions_gbt.groupby('indexedLabel', 'prediction').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictionAndLabels = predictions_gbt.select(\"indexedLabel\", \"prediction\").rdd\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Area under ROC = %g\" % metrics.areaUnderROC)\n",
    "print(\"Area under PR = %g\\n\" % metrics.areaUnderPR)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\")\n",
    "print(\"Accuracy = %g\" % evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"accuracy\"}))\n",
    "print(\"Weighted Precision = %g\" % evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "print(\"Weighted Recall = %g\" % evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"weightedRecall\"}))\n",
    "print(\"F1 = %g\" % evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"f1\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn to calculate the raw precision and recall\n",
    "\n",
    "gbt_result = predictions_gbt.select('indexedLabel', 'prediction').toPandas()\n",
    "\n",
    "gbt_label = gbt_result['indexedLabel'].tolist()\n",
    "gbt_prediction = gbt_result['prediction'].tolist()\n",
    "\n",
    "precision, recall, fscore, support = score(gbt_label, gbt_prediction)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing results from Random Forest and Gradient Boosted Tree:\n",
    "-  Gradient Boosted Tree gives better recall but worse precision compared with Random Forest. For most of the predictive maintenance use cases, the business cost associated with false positives is usually expensive. There is always a trade-off between precision and recall. We want to achieve higher precision rate (fewer false positives) even though that might compromise the recall rate.\n",
    "-  That is why we decided to go with Random Forest model and further optimized it with hyper-parametre tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning & Cross Validation\n",
    "\n",
    "Train a random forest classification model using hyper-parameter tuning and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", \n",
    "                            featureSubsetStrategy=\"auto\", impurity=\"gini\", seed=123)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
    "\n",
    "\n",
    "## Define parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50, 100]) \\\n",
    "    .addGrid(rf.maxBins, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "## Define cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName=\"weightedPrecision\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "## Train model using CV\n",
    "cvModel = crossval.fit(train_downsampled)\n",
    "\n",
    "## Predict and evaluate\n",
    "predictions = cvModel.transform(testing)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"weightedPrecision on test data = %g\" % r2)\n",
    "\n",
    "## Save the best model\n",
    "fileName = \"CV_RandomForestClassificationModel_\" + datestamp;\n",
    "CVDirfilename = modelDir + fileName + suffix;\n",
    "cvModel.bestModel.save(CVDirfilename);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning only improved the model performance a little bit. We will then use that model for future scoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
