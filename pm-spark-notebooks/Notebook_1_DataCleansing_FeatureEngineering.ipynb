{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #1\n",
    "\n",
    "In this notebook, we show how to import, clean and create features relevent for predictive maintenance data using PySpark. This notebook uses Spark **2.0.2** and Python Python **2.7.5**. The API documentation for that version can be found [here](https://spark.apache.org/docs/2.0.2/api/python/index.html).\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Import Data](#Import-Data)\n",
    "- [Data Exploration & Cleansing](#Data-Exploration-&-Cleansing)\n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "- [Save Result](#Save-Result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import atexit\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import concat, col, udf, lag, date_add, explode, lit, unix_timestamp\n",
    "from pyspark.sql.functions import month, weekofyear, dayofmonth\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.dataframe import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.feature import StandardScaler, PCA, RFormula\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "start_time = time.time()\n",
    "base_path = \"/home/jovyan/data/\"\n",
    "suffix = \"_1000\"\n",
    "\n",
    "spark = ( SparkSession\n",
    "  .builder\n",
    "  .master(\"local[30]\")\n",
    "  .appName(\"pm\")\n",
    "  .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark.conf.set(\"spark.executor.memory\", \"Xg\")\n",
    "spark.conf.set(\"spark.driver.memory\", \"Xg\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import data from Azure Blob Storage\n",
    "dataFile = base_path + 'sampledata' + suffix +'.csv'\n",
    "dataFileSep = ','\n",
    "df = spark.read.csv(dataFile, header=True, sep=dataFileSep, inferSchema=True, nanValue=\"\", mode='PERMISSIVE')\n",
    "\n",
    "# Import data from the home directory on your machine \n",
    "# dataFile = '/home/katlin/Desktop/PysparkExample/sampledata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the dataset dimension and data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of the data\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the issue of df.show() is fixed\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check data schema\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanations on the data schema:\n",
    "* ***DeviceID***: machine identifier\n",
    "* ***Date***: the day when that row of data was collected for that machine\n",
    "* ***Categorical_1 to 4***: some categorical features about the machine\n",
    "* ***Problem_Type_1 to 4***: the total number of times Problem type 1 (2, 3, 4) occured on that day for that machine\n",
    "* ***Usage_Count_1 (2)***: the total number of times that machine had been used on that day for purpose type 1 or 2\n",
    "* ***Warning_xxx***: the total number of Warning type_xxx occured for that machine on that day\n",
    "* ***Error_Count_1 to 8***: the total number of times Error type 1 (to 8) occured on that day for that machine\n",
    "* ***Fault_Code_Type_1 to 4***: fault code type 1 (2, 3, 4) occured on that day for that machine\n",
    "* ***Problemreported***: prediction target column whether or not there is a machine problem on that day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As part of the data cleansing process, we standardized all the column names to lower case and replaced all the symbols with underscore. We also removed any duplicated records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------- initial data cleansing ---------------------------------------------#\n",
    "\n",
    "# standardize the column names\n",
    "def StandardizeNames(df):\n",
    "    l = df.columns\n",
    "    cols = [c.replace(' ','_').\n",
    "              replace('[.]','_').\n",
    "              replace('.','_').\n",
    "              replace('[[:punct:]]','_').\n",
    "              lower() for c in l]\n",
    "    return df.toDF(*cols)\n",
    "df = StandardizeNames(df)\n",
    "\n",
    "# remove duplicated rows based on deviceid and date\n",
    "df = df.dropDuplicates(['deviceid', 'date'])\n",
    "\n",
    "# remove rows with missing deviceid, date\n",
    "df = df.dropna(how='any', subset=['deviceid', 'date'])\n",
    "\n",
    "df.select('deviceid','date').show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define groups of features -- date, categorical, numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------- Define groups of features -----------------------------------------#\n",
    "\n",
    "features_datetime = ['date']\n",
    "\n",
    "features_categorical = ['deviceid','Categorical_1','Categorical_2','Categorical_3','Categorical_4',\n",
    "                        'fault_code_type_1','fault_code_type_2',\n",
    "                        'fault_code_type_3','fault_code_type_4',\n",
    "                        'problemreported']\n",
    "\n",
    "features_numeric = list(set(df.columns) -set(features_datetime)-set(features_categorical))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['fault_code_type_3',].head(3))\n",
    "# there are some missing values, we need to handle in the subsequent steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# handle missing values\n",
    "df = df.fillna(0, subset=features_numeric)\n",
    "df = df.fillna(\"Unknown\", subset=features_categorical)\n",
    "\n",
    "# check the results\n",
    "print(df['fault_code_type_3',].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For data exploration part, people usually would like to visualize the distribution of certain columns or the interation among columns. Here, we hand picked some columns to demonstrate how to do some basic visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------ data exploration and visualization ------------------------------------#\n",
    "\n",
    "# Register dataframe as a temp table in SQL context\n",
    "df.createOrReplaceTempView(\"df1\")\n",
    "\n",
    "sqlStatement = \"\"\"\n",
    "    SELECT problem_type_1, problem_type_2, problem_type_3, problem_type_4,\n",
    "    error_count_1, error_count_2, error_count_3, error_count_4, \n",
    "    error_count_5, error_count_6, error_count_7, error_count_8, problemreported\n",
    "    FROM df1\n",
    "\"\"\"\n",
    "plotdata = spark.sql(sqlStatement).toPandas();\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# show histogram distribution of some features\n",
    "ax1 = plotdata[['problem_type_1']].plot(kind='hist', bins=5, facecolor='blue')\n",
    "ax1.set_title('problem_type_1 distribution')\n",
    "ax1.set_xlabel('number of problem_type_1 per day'); ax1.set_ylabel('Counts');\n",
    "plt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n",
    "\n",
    "ax1 = plotdata[['problem_type_2']].plot(kind='hist', bins=5, facecolor='blue')\n",
    "ax1.set_title('problem_type_2 distribution')\n",
    "ax1.set_xlabel('number of problem_type_2 per day'); ax1.set_ylabel('Counts');\n",
    "plt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n",
    "\n",
    "\n",
    "# show correlation matrix heatmap to explore some potential interesting patterns\n",
    "corr = plotdata.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "In the remaining part of the Notebook #1, we will demonstrate how to generate new features for this kind of use case. It is definitely not meant to be a comprehensive list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we created some time features, calculated the total number of warning_type1 (type2) occured for a macine on a particular day. We also identified some data quality issue that some event counts had negative values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract some time features from \"date\" column\n",
    "df = df.withColumn('month', month(df['date']))\n",
    "df = df.withColumn('weekofyear', weekofyear(df['date']))\n",
    "df = df.withColumn('dayofmonth', dayofmonth(df['date']))\n",
    "\n",
    "\n",
    "# warning related raw features\n",
    "warning_type1_features = list(s for s in df.columns if \"warning_1_\" in s) \n",
    "                            \n",
    "warning_type2_features = list(s for s in df.columns if \"warning_2_\" in s)\n",
    "\n",
    "warning_all = warning_type1_features + warning_type2_features\n",
    "\n",
    "# total count of all type1 warnings each day each device\n",
    "df = df.withColumn('warn_type1_total', F.UserDefinedFunction(lambda *args: sum(args), IntegerType())(*warning_type1_features))\n",
    "\n",
    "# total count of all type2 warnings each day each device\n",
    "df = df.withColumn('warn_type2_total', F.UserDefinedFunction(lambda *args: sum(args), IntegerType())(*warning_type1_features))\n",
    "\n",
    "print(df['warn_type1_total',].head(3))\n",
    "print(df['warn_type2_total',].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We realized that the warning counts have negative values\n",
    "# Replace all the negative values with 0\n",
    "\n",
    "def negative_replace(num):\n",
    "    if num < 0: return 0\n",
    "    else: return num\n",
    "    \n",
    "negative_replace_Udf = udf(negative_replace, IntegerType())\n",
    "\n",
    "m = warning_type1_features + warning_type2_features\n",
    "for col_n in m:\n",
    "    df = df.withColumn(col_n, negative_replace_Udf(df[col_n]))\n",
    "\n",
    "# Then we have to re-calculate the total warnings again \n",
    "df = df.withColumn('warn_type1_total', F.UserDefinedFunction(lambda *args: sum(args), IntegerType())(*warning_type1_features))\n",
    "df = df.withColumn('warn_type2_total', F.UserDefinedFunction(lambda *args: sum(args), IntegerType())(*warning_type2_features))\n",
    "\n",
    "print(df['warn_type1_total',].head(3))\n",
    "print(df['warn_type2_total',].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables \"categorical_1 to 4\" are integer type but in fact they are categorical features. In the following cell, we binned those variables and created four new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we can also use SparkSQL for this binning task\n",
    "\n",
    "def Cat1(num):\n",
    "    if num <= 10: return '0-10'\n",
    "    elif 10 < num and num <= 20: return '11-20'\n",
    "    elif 20 < num and num <= 30: return '21-30'\n",
    "    elif 30 < num and num <= 40: return '31-40'\n",
    "    else: return 'morethan40'\n",
    "cat1Udf = udf(Cat1, StringType())\n",
    "df = df.withColumn(\"cat1\", cat1Udf('categorical_1'))\n",
    "\n",
    "\n",
    "def Cat2(num):\n",
    "    if num <= 2000: return '0-2000'\n",
    "    elif 2000 < num and num <= 3000: return '2000-3000'\n",
    "    elif 3000 < num and num <= 4000: return '3000-4000'\n",
    "    elif 4000 < num and num <= 5000: return '4000-5000'\n",
    "    elif 5000 < num and num <= 6000: return '5000-6000'\n",
    "    else: return 'morethan6000'\n",
    "cat2Udf = udf(Cat2, StringType())\n",
    "df = df.withColumn(\"cat2\", cat2Udf('categorical_2'))\n",
    "\n",
    "\n",
    "def Cat3(num):\n",
    "    if num <= 200: return '0-200'\n",
    "    elif 200 < num and num <= 400: return '200-400'\n",
    "    elif 400 < num and num <= 600: return '400-600'\n",
    "    elif 600 < num and num <= 800: return '600-800'\n",
    "    else: return 'morethan800'\n",
    "cat3Udf = udf(Cat3, StringType())\n",
    "df = df.withColumn(\"cat3\", cat3Udf('categorical_3'))\n",
    "\n",
    "\n",
    "def Cat4(num):\n",
    "    if num <= 5000: return '0-5000'\n",
    "    elif 5000 < num and num <= 10000: return '5000-10000'\n",
    "    elif 10000 < num and num <= 15000: return '10000-15000'\n",
    "    elif 15000 < num and num <= 20000: return '15000-20000'\n",
    "    else: return 'morethan20000'\n",
    "cat4Udf = udf(Cat4, StringType())\n",
    "df = df.withColumn(\"cat4\", cat4Udf('categorical_4'))\n",
    "\n",
    "\n",
    "print(df.select('cat1').distinct().rdd.map(lambda r: r[0]).collect())\n",
    "print(df.select('cat2').distinct().rdd.map(lambda r: r[0]).collect())\n",
    "print(df.select('cat3').distinct().rdd.map(lambda r: r[0]).collect())\n",
    "print(df.select('cat4').distinct().rdd.map(lambda r: r[0]).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For variables \"fault_code_type_1 to 4\", if it is \"Unknown\" that means there is \"0\" fault code reported on that day for that machine, otherwise the count of fault code type 1 (2, 3, or 4) is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"fault_code_type_1_count\",F.when(df.fault_code_type_1!= \"Unknown\", 1).otherwise(0))\\\n",
    "       .withColumn(\"fault_code_type_2_count\",F.when(df.fault_code_type_2!= \"Unknown\", 1).otherwise(0))\\\n",
    "       .withColumn(\"fault_code_type_3_count\",F.when(df.fault_code_type_3!= \"Unknown\", 1).otherwise(0))\\\n",
    "       .withColumn(\"fault_code_type_4_count\",F.when(df.fault_code_type_4!= \"Unknown\", 1).otherwise(0))\n",
    "\n",
    "df.groupby('fault_code_type_1_count').count().show()\n",
    "df.groupby('fault_code_type_2_count').count().show()\n",
    "df.groupby('fault_code_type_3_count').count().show()\n",
    "df.groupby('fault_code_type_4_count').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering performance related features\n",
    "We first select 8 raw performance features to be normalized and then select 2 normalizers.  \n",
    "The idea behind this normalization is that device with more problem/error/fault reported might simply because it is used more frequently. Therefore, we need to normalize the problem counts by the corresponding usage counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, select the 8 raw performance features to be normalized\n",
    "performance_normal_raw = ['problem_type_1','problem_type_2','problem_type_3','problem_type_4',\n",
    "                          'fault_code_type_1_count','fault_code_type_2_count',\n",
    "                          'fault_code_type_3_count', 'fault_code_type_4_count']\n",
    "\n",
    "# Then, select 2 normalizers\n",
    "performance_normalizer = ['usage_count_1','usage_count_2']\n",
    "\n",
    "# Normalize performance_normal_raw by \"usage_count_1\"\n",
    "df = df.withColumn(\"problem_type_1_per_usage1\", F.when(df.usage_count_1==0,0).otherwise(df.problem_type_1/df.usage_count_1))\\\n",
    "       .withColumn(\"problem_type_2_per_usage1\",F.when(df.usage_count_1==0,0).otherwise(df.problem_type_2/df.usage_count_1))\\\n",
    "       .withColumn(\"problem_type_3_per_usage1\",F.when(df.usage_count_1==0,0).otherwise(df.problem_type_3/df.usage_count_1))\\\n",
    "       .withColumn(\"problem_type_4_per_usage1\",F.when(df.usage_count_1==0,0).otherwise(df.problem_type_4/df.usage_count_1))\\\n",
    "       .withColumn(\"fault_code_type_1_count_per_usage1\",F.when(df.usage_count_1==0,0).otherwise(df.fault_code_type_1_count/df.usage_count_1))\\\n",
    "       .withColumn(\"fault_code_type_2_count_per_usage1\",F.when(df.usage_count_1==0,0).otherwise(df.fault_code_type_2_count/df.usage_count_1))\\\n",
    "       .withColumn(\"fault_code_type_3_count_per_usage1\",F.when(df.usage_count_1==0,0).otherwise(df.fault_code_type_3_count/df.usage_count_1))\\\n",
    "       .withColumn(\"fault_code_type_4_count_per_usage1\",F.when(df.usage_count_1==0,0).otherwise(df.fault_code_type_4_count/df.usage_count_1))\n",
    "\n",
    "# Normalize performance_normal_raw by \"usage_count_2\"\n",
    "df = df.withColumn(\"problem_type_1_per_usage2\", F.when(df.usage_count_2==0,0).otherwise(df.problem_type_1/df.usage_count_2))\\\n",
    "       .withColumn(\"problem_type_2_per_usage2\",F.when(df.usage_count_2==0,0).otherwise(df.problem_type_2/df.usage_count_2))\\\n",
    "       .withColumn(\"problem_type_3_per_usage2\",F.when(df.usage_count_2==0,0).otherwise(df.problem_type_3/df.usage_count_2))\\\n",
    "       .withColumn(\"problem_type_4_per_usage2\",F.when(df.usage_count_2==0,0).otherwise(df.problem_type_4/df.usage_count_2))\\\n",
    "       .withColumn(\"fault_code_type_1_count_per_usage2\",F.when(df.usage_count_2==0,0).otherwise(df.fault_code_type_1_count/df.usage_count_2))\\\n",
    "       .withColumn(\"fault_code_type_2_count_per_usage2\",F.when(df.usage_count_2==0,0).otherwise(df.fault_code_type_2_count/df.usage_count_2))\\\n",
    "       .withColumn(\"fault_code_type_3_count_per_usage2\",F.when(df.usage_count_2==0,0).otherwise(df.fault_code_type_3_count/df.usage_count_2))\\\n",
    "       .withColumn(\"fault_code_type_4_count_per_usage2\",F.when(df.usage_count_2==0,0).otherwise(df.fault_code_type_4_count/df.usage_count_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar to what we did for \"categorical_1 to 4\", in the following cell we binned performance related features and created new categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of performance related features which we would like to perform binning\n",
    "c_names = ['problem_type_1', 'problem_type_3', 'problem_type_4',\n",
    "           'problem_type_1_per_usage1','problem_type_2_per_usage1','problem_type_3_per_usage1','problem_type_4_per_usage1',\n",
    "           'problem_type_1_per_usage2','problem_type_2_per_usage2','problem_type_3_per_usage2','problem_type_4_per_usage2',\n",
    "           'fault_code_type_1_count', 'fault_code_type_2_count', 'fault_code_type_3_count', 'fault_code_type_4_count',                          \n",
    "           'fault_code_type_1_count_per_usage1','fault_code_type_2_count_per_usage1',\n",
    "           'fault_code_type_3_count_per_usage1', 'fault_code_type_4_count_per_usage1',\n",
    "           'fault_code_type_1_count_per_usage2','fault_code_type_2_count_per_usage2',\n",
    "           'fault_code_type_3_count_per_usage2', 'fault_code_type_4_count_per_usage2']\n",
    "\n",
    "# Bin size ('0','1','>1') for most of the performance features because majority of the values fall into the range of 0 to slightly more than 1.\n",
    "def performanceCat(num):\n",
    "    if num == 0: return '0'\n",
    "    elif num ==1: return '1'\n",
    "    else: return '>1'\n",
    "    \n",
    "performanceCatUdf = udf(performanceCat, StringType())\n",
    "for col_n in c_names:\n",
    "    df = df.withColumn(col_n+'_category',performanceCatUdf(df[col_n]))\n",
    "\n",
    "# Use different bin for \"problem_type_2\" because we saw a larger spread of the values\n",
    "def problem_type_2_Cat(num):\n",
    "    if num == 0: return '0'\n",
    "    elif 0 < num and num <= 5: return '1-5'\n",
    "    elif 5 < num and num <= 10: return '6-10'\n",
    "    else: return '>10'\n",
    "\n",
    "problem_type_2_CatUdf = udf(problem_type_2_Cat, StringType())\n",
    "df = df.withColumn('problem_type_2_category',problem_type_2_CatUdf(df['problem_type_2']))\n",
    "\n",
    "\n",
    "print(df.select('problem_type_1_category').distinct().rdd.map(lambda r: r[0]).collect())\n",
    "print(df.select('problem_type_2_category').distinct().rdd.map(lambda r: r[0]).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encode some categotical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define the list of categorical features\n",
    "\n",
    "catVarNames = ['problem_type_1_category', 'problem_type_2_category',\n",
    "               'problem_type_3_category', 'problem_type_4_category',\n",
    "               'problem_type_1_per_usage1_category', 'problem_type_2_per_usage1_category',\n",
    "               'problem_type_3_per_usage1_category', 'problem_type_4_per_usage1_category',\n",
    "               'problem_type_1_per_usage2_category', 'problem_type_2_per_usage2_category',\n",
    "               'problem_type_3_per_usage2_category', 'problem_type_4_per_usage2_category',\n",
    "               'fault_code_type_1_count_category', 'fault_code_type_2_count_category',\n",
    "               'fault_code_type_3_count_category', 'fault_code_type_4_count_category',\n",
    "               'fault_code_type_1_count_per_usage1_category', 'fault_code_type_2_count_per_usage1_category',\n",
    "               'fault_code_type_3_count_per_usage1_category', 'fault_code_type_4_count_per_usage1_category',\n",
    "               'fault_code_type_1_count_per_usage2_category', 'fault_code_type_2_count_per_usage2_category',\n",
    "               'fault_code_type_3_count_per_usage2_category', 'fault_code_type_4_count_per_usage2_category',\n",
    "               'cat1','cat2','cat3','cat4']\n",
    "    \n",
    "    \n",
    "sIndexers = [StringIndexer(inputCol=x, outputCol=x + '_indexed') for x in catVarNames]\n",
    "\n",
    "df_cat = Pipeline(stages=sIndexers).fit(df).transform(df)\n",
    "\n",
    "# Remove columns with only 1 level (compute variances of columns)\n",
    "catColVariance = df_cat.select(\n",
    "    *(F.variance(df_cat[c]).alias(c + '_sd') for c in [cv + '_indexed' for cv in catVarNames]))\n",
    "catColVariance = catColVariance.rdd.flatMap(lambda x: x).collect()\n",
    "catVarNames = [catVarNames[k] for k in [i for i, v in enumerate(catColVariance) if v != 0]]\n",
    "\n",
    "# Encode\n",
    "ohEncoders = [OneHotEncoder(inputCol=x + '_indexed', outputCol=x + '_encoded')\n",
    "              for x in catVarNames]\n",
    "ohPipelineModel = Pipeline(stages=ohEncoders).fit(df_cat)\n",
    "df_cat = ohPipelineModel.transform(df_cat)\n",
    "\n",
    "drop_list = [col_n for col_n in df_cat.columns if 'indexed' in col_n]\n",
    "df = df_cat.select([column for column in df_cat.columns if column not in drop_list])\n",
    "\n",
    "print(df['problem_type_1_category_encoded',].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use PCA to reduce number of features\n",
    "In Notebook #2, we will perform a series of rolling computation for various features, time windows and aggregated statistics. This process is very computational expensive and therefore we need to first reduce the feature list.  \n",
    "In the dataset, there are many warning related features and most of them have value of 0 so quite sparse. We can group or find correlations among those warning features, reduce the feature space for downstream work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the number of warning related features\n",
    "len([col_n for col_n in df.columns if 'warning' in col_n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "#----------------------------- PCA feature grouping on warning related features --------------------------#\n",
    "\n",
    "df = df.withColumn(\"key\", concat(df.deviceid,lit(\"_\"),df.date))\n",
    "\n",
    "# step 1\n",
    "# Use RFormula to create the feature vector\n",
    "formula = RFormula(formula = \"~\" + \"+\".join(warning_all))\n",
    "output = formula.fit(df).transform(df).select(\"key\",\"features\") \n",
    "\n",
    "\n",
    "# step 2\n",
    "# Before PCA, we need to standardize the features, it is very important...\n",
    "# Note that StandardScaler does not work for sparse vector unless withMean=false\n",
    "# OR we can convert sparse vector to dense vector first using toArray\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(output)\n",
    "\n",
    "\n",
    "# step 3\n",
    "pca = PCA(k=20, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(scaledData)\n",
    "result = model.transform(scaledData).select(\"key\",\"pcaFeatures\")\n",
    "\n",
    "# to check how much variance explained by each component\n",
    "print(model.explainedVariance)  \n",
    "\n",
    "\n",
    "# step 4\n",
    "# convert pca result, a vector column, to mulitple columns\n",
    "# The reason why we did this was because later on we need to use those columns to generate more features (rolling compute) \n",
    "def extract(row):\n",
    "    return (row.key, ) + tuple(float(x) for x in row.pcaFeatures.values)\n",
    "\n",
    "pca_outcome = result.rdd.map(extract).toDF([\"key\"])\n",
    "\n",
    "# rename columns of pca_outcome\n",
    "oldColumns = pca_outcome.schema.names\n",
    "\n",
    "newColumns = [\"key\", \n",
    "              \"pca_1_warn\",\"pca_2_warn\",\"pca_3_warn\",\"pca_4_warn\",\"pca_5_warn\",\n",
    "              \"pca_6_warn\",\"pca_7_warn\",\"pca_8_warn\",\"pca_9_warn\",\"pca_10_warn\",\n",
    "              \"pca_11_warn\",\"pca_12_warn\",\"pca_13_warn\",\"pca_14_warn\",\"pca_15_warn\",\n",
    "              \"pca_16_warn\",\"pca_17_warn\",\"pca_18_warn\",\"pca_19_warn\",\"pca_20_warn\",\n",
    "             ]\n",
    "\n",
    "pca_result = reduce(lambda pca_outcome, idx: pca_outcome.withColumnRenamed(oldColumns[idx], newColumns[idx]), \\\n",
    "                                        range(len(oldColumns)), pca_outcome)\n",
    "\n",
    "df = df.join(pca_result, 'key', 'inner')\n",
    "\n",
    "print(df['pca_1_warn',].head(3))\n",
    "\n",
    "warning_drop_list = [col_n for col_n in df.columns if 'warning_' in col_n]\n",
    "df = df.select([column for column in df.columns if column not in warning_drop_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Result\n",
    "\n",
    "Due to the lazy compute of Spark, it is usually more efficient to break down the workload into chunks and materialize the intermediate results. For example, we divided the tutorial into three notebooks, the result from Notebook #1 would be used as input data for Notebook #2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df.write.mode('overwrite').parquet(base_path + 'tmp/notebook1_result' + suffix + '.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
