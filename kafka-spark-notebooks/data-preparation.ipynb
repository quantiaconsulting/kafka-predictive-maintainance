{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "val base_path = \"...\"\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"pm\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val error_schema = new StructType()\n",
    "      .add(\"deviceID\",StringType)\n",
    "      .add(\"timestamp\",StringType)\n",
    "      .add(\"error\",StringType)\n",
    "\n",
    "val warning_schema = new StructType()\n",
    "      .add(\"deviceID\",StringType)\n",
    "      .add(\"timestamp\",StringType)\n",
    "      .add(\"warning\",StringType)\n",
    "\n",
    "val get_tsID = (ts: Long) => {\n",
    "    val startTs = 1262304000\n",
    "    val interval = 900\n",
    "    Math.round(((ts - startTs) / interval) + 1)\n",
    "}\n",
    "\n",
    "val get_tsID_UDF = udf(get_tsID)\n",
    "\n",
    "val errors_df = spark\n",
    "    .readStream\n",
    "    .schema(error_schema)\n",
    "    .parquet(base_path + \"errors.parquet\")\n",
    "    .withColumnRenamed(\"timestamp\", \"unix_ts\")\n",
    "    .withColumn(\"timestamp\", from_unixtime($\"unix_ts\" / 1000))\n",
    "    .withColumn(\"timestamp\", $\"timestamp\".cast(\"timestamp\"))\n",
    "    .withColumn(\"tsID\", get_tsID_UDF($\"unix_ts\"))\n",
    "\n",
    "\n",
    "val warnings_df = spark\n",
    "    .readStream\n",
    "    .schema(warning_schema)    \n",
    "    .parquet(base_path + \"warnings.parquet\")\n",
    "    .withColumnRenamed(\"timestamp\", \"unix_ts\")\n",
    "    .withColumn(\"timestamp\", from_unixtime($\"unix_ts\" / 1000))\n",
    "    .withColumn(\"timestamp\", $\"timestamp\".cast(\"timestamp\"))\n",
    "    .withColumn(\"tsID\", get_tsID_UDF($\"unix_ts\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val errors_wtk = errors_df.withWatermark(\"timestamp\", \"15 minutes\")\n",
    "val warnings_wtk = warnings_df.withWatermark(\"timestamp\", \"15 minutes\")\n",
    "\n",
    "val errors_warnings = errors_wtk.as(\"err\").join(\n",
    "  warnings_wtk.as(\"war\"),\n",
    "  expr(\"\"\"\n",
    "    err.deviceID = war.deviceID AND\n",
    "    err.tsID = war.tsID\n",
    "    \"\"\")\n",
    ")\n",
    ".select(\"err.*\", \"war.warning\")\n",
    "\n",
    "errors_warnings\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"parquet\")\n",
    "  .option(\"checkpointLocation\", base_path + \"checkpoint/ew/\")\n",
    "  .option(\"path\", base_path + \"ew.parquet\")\n",
    "  .start()\n",
    "  .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
